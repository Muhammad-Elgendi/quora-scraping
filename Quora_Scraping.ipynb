{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkfVr2F2usRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I tried  this package https://pypi.org/project/quora-scraper/  \n",
        "# and after going through installation steps \n",
        "# I faced some problems so I copied the code from the package\n",
        "# and made some modifications"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxhnjch1yisB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "f9c6422a-a9d8-48b7-cc12-6629167d56d2"
      },
      "source": [
        "# Mount the Google drive\n",
        "from google.colab import drive # this will be our driver\n",
        "drive.mount('/gdrive')\n",
        "root = '/gdrive/My Drive/'     # if you want to operate on your Google Drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6LBmkP329qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d226eef9-1960-457d-c9ed-5a7a9ff01861"
      },
      "source": [
        "# install selenium package\n",
        "!pip install selenium"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd0KTNGe3BlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "d2f113cf-c327-4c05-edc4-8737492f47d5"
      },
      "source": [
        "# Download Chromedriver because this package require it\n",
        "!wget https://chromedriver.storage.googleapis.com/85.0.4183.38/chromedriver_linux64.zip  && unzip chromedriver_linux64"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-07 23:12:04--  https://chromedriver.storage.googleapis.com/85.0.4183.38/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 173.194.217.128, 2607:f8b0:400c:c13::80\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|173.194.217.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5358213 (5.1M) [application/zip]\n",
            "Saving to: ‘chromedriver_linux64.zip’\n",
            "\n",
            "\rchromedriver_linux6   0%[                    ]       0  --.-KB/s               \rchromedriver_linux6 100%[===================>]   5.11M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-08-07 23:12:04 (158 MB/s) - ‘chromedriver_linux64.zip’ saved [5358213/5358213]\n",
            "\n",
            "Archive:  chromedriver_linux64.zip\n",
            "  inflating: chromedriver            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQv7PB_90FZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We set a variable with the *Colab* default path and chrome driver path, to easily use it later\n",
        "colab_path = '/content/'\n",
        "chromedriver_path = '/content/chromedriver'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBndhH9F5SFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add chromedriver to sys path\n",
        "import sys\n",
        "sys.path.insert(0,chromedriver_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foh2N5-u5nvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "94836919-92f6-462d-eb25-69759e11a83f"
      },
      "source": [
        "# show sys path\n",
        "print(sys.path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/chromedriver', '', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MPHvWk74LI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try chrome driver and adjusting some options such that it does not crash in google colab\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LylpRZx86Q85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5acebe6c-06bf-4b1a-ab28-4f41e6fe25c2"
      },
      "source": [
        "# test webdriver\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "driver.get(\"https://quantamixsolutions.com/\")\n",
        "driver.get_screenshot_as_file(\"screenshot.png\")\n",
        "print(\"We are in \",driver.title)\n",
        "driver.quit()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are in  Become Your Market's Leading Authority on Page #1 of Google | Quantamix Solutions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaovka0d3eHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modified version of https://github.com/banyous/quora-scraper/blob/master/quora_scraper/scraper.py\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import ast\n",
        "import csv\n",
        "import json\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import random\n",
        "import userpaths\n",
        "import dateparser\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "# -------------------------------------------------------------\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def connectchrome():\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  chrome_options.add_argument(\"--start-maximized\")\n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "  return driver\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# -------------------------------------------------------------\n",
        "# remove 'k'(kilo) and 'm'(million) from Quora numbers\n",
        "def convertnumber(number):\n",
        "\tif 'k' in number:\n",
        "\t\tn=float(number.lower().replace('k', '').replace(' ', ''))*1000\n",
        "\telif 'm' in number:\n",
        "\t\tn=float(number.lower().replace('m', '').replace(' ', ''))*1000000\n",
        "\telse:\n",
        "\t\tn=number\n",
        "\treturn int(n)\n",
        "# -------------------------------------------------------------\n",
        "# -------------------------------------------------------------\n",
        "# convert Quora dates (such as 2 months ago) to DD-MM-YYYY format\n",
        "def convertDateFormat(dateText):\n",
        "\ttry:\n",
        "\t\tif (\"Updated\" in dateText):\n",
        "\t\t\tdate = dateText[8:]\n",
        "\t\telse:\n",
        "\t\t\tdate = dateText[9:]\n",
        "\t\tdate = dateparser.parse(dateText).strftime(\"%Y-%m-%d\")\n",
        "\texcept:  # when updated or answered in the same week (ex: Updated Sat)\n",
        "\t\tdate = dateparser.parse(\"7 days ago\").strftime(\"%Y-%m-%d\")\n",
        "\treturn date\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# -------------------------------------------------------------\n",
        "def scrollup_alittle(self,nbtimes):\n",
        "\t\n",
        "\tfor iii in range(0,nbtimes):\n",
        "\t\tself.execute_script(\"window.scrollBy(0,-200)\")\n",
        "\t\ttime.sleep(1)\n",
        "# -------------------------------------------------------------\n",
        "# -------------------------------------------------------------\n",
        "# method for loading  quora dynamic content\n",
        "def scrolldown(self,type_of_page='users'):\n",
        "\tlast_height = self.page_source\n",
        "\tloop_scroll=True\n",
        "\tattempt = 0\n",
        "\t# we generate a random waiting time between 2 and 4\n",
        "\twaiting_scroll_time=round(random.uniform(2, 4),1)\n",
        "\tprint('scrolling down to get all answers...')\n",
        "\tmax_waiting_time=round(random.uniform(5, 7),1)\n",
        "\t# we increase waiting time when we look for questions urls\t\n",
        "\tif type_of_page=='questions' : max_waiting_time= round(random.uniform(20, 30),1)\n",
        "\t# scroll down loop until page not changing\n",
        "\twhile loop_scroll:\n",
        "\t\tself.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\t\ttime.sleep(2)\n",
        "\t\tif type_of_page=='answers':\n",
        "\t\t\tscrollup_alittle(self,2)\n",
        "\t\tnew_height=self.page_source\n",
        "\t\tif new_height == last_height:\n",
        "\t\t\t# in case of not change, we increase the waiting time\n",
        "\t\t\twaiting_scroll_time= max_waiting_time\n",
        "\t\t\tattempt += 1\n",
        "\t\t\tif attempt==3:# in the third attempt we end the scrolling\n",
        "\t\t\t\tloop_scroll=False\n",
        "\t\t\t#print('attempt',attempt)\n",
        "\t\telse:\n",
        "\t\t\tattempt=0\n",
        "\t\t\twaiting_scroll_time=round(random.uniform(2, 4),1)\n",
        "\t\tlast_height=new_height\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# -------------------------------------------------------------\t\n",
        "# questions urls crawler \n",
        "def questions(topics_list,save_path):\n",
        "\tbrowser=connectchrome()\n",
        "\ttopic_index=-1\n",
        "\tloop_limit=len(topics_list)\n",
        "\tprint('Starting the questions crawling')\n",
        "\twhile True:\n",
        "\t\tprint('--------------------------------------------------')\n",
        "\t\ttopic_index += 1\n",
        "\t\tif topic_index>=loop_limit:\n",
        "\t\t\tprint('Crawling completed, questions have been saved to  :  ', save_path)\n",
        "\t\t\tbrowser.quit()\n",
        "\t\t\tbreak\n",
        "\t\ttopic_term = topics_list[topic_index].strip()\n",
        "\t\t# we remove hashtags (optional)\n",
        "\t\ttopic_term.replace(\"#\",'')\n",
        "\t\t# Looking if the topic has an existing Quora url\n",
        "\t\tprint('#########################################################')\n",
        "\t\tprint('Looking for topic number : ',topic_index,' | ', topic_term)\n",
        "\t\ttry:\n",
        "\t\t\turl = \"https://www.quora.com/topic/\" + topic_term.strip() + \"/all_questions\"\n",
        "\t\t\tbrowser.get(url)\n",
        "\t\t\ttime.sleep(2)\n",
        "\t\texcept Exception as e0:\n",
        "\t\t\tprint('topic does not exist in Quora')\n",
        "\t\t\t# print('exception e0')\n",
        "\t\t\t# print('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e0).__name__, e0)\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\t# get browser source\n",
        "\t\thtml_source = browser.page_source\n",
        "\t\tquestion_count_soup = BeautifulSoup(html_source, 'html.parser')\n",
        "\n",
        "\t\t#  get total number of questions\n",
        "\t\tquestion_count_str = question_count_soup.find('a', attrs={'class': 'TopicQuestionsStatsRow'})\n",
        "\t\tif str(question_count_str) =='None':\n",
        "\t\t\tprint('topic does not have questions...')\n",
        "\t\t\tcontinue\n",
        "\t\tquestion_count = convertnumber(question_count_str.contents[0].text)\n",
        "\t\tquestion_count_str = question_count_soup.find('a', attrs={'class': 'TopicQuestionsStatsRow'})\n",
        "\t\tif question_count ==0:\n",
        "\t\t\tprint('topic does not have questions...')\n",
        "\t\t\tcontinue\n",
        "\t\tprint('number of questions for this topic : '+ str(question_count))\n",
        "\n",
        "\t\t# Get scroll height\n",
        "\t\tlast_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "\t\t# infinite while loop, break it when you reach the end of the page or not able to scroll further.\n",
        "\t\t# Note that Quora\n",
        "\t\t# if there is more than 10 questions, we need to scroll down the profile to load remaining questions\n",
        "\t\tif int(question_count)>10: \n",
        "\t\t\tscrolldown(browser,'questions')\n",
        "\n",
        "\t\t# next we harvest all questions URLs that exists in the Quora topic's page\n",
        "\t\t# get html page source\n",
        "\t\thtml_source = browser.page_source\n",
        "\t\tsoup = BeautifulSoup(html_source, 'html.parser')\n",
        "\n",
        "\t\t# question_link is the class for questions\n",
        "\t\tquestion_link = soup.find_all('a', attrs={'class': 'question_link'}, href=True)\n",
        "\n",
        "\t\t# add questions to a set for uniqueness\n",
        "\t\tquestion_set = set()\n",
        "\t\tfor ques in question_link:\n",
        "\t\t\tquestion_set.add(ques)\n",
        "\n",
        "\t\t# write content of set to Qyestions_URLs/ folder\n",
        "\t\tsave_file= Path(save_path) /  str(topic_term.strip('\\n') + '_question_urls.txt')\n",
        "\t\tfile_question_urls = open(save_file, mode='w', encoding='utf-8')\n",
        "\t\tfor ques in question_set:\n",
        "\t\t\tlink_url = \"http://www.quora.com\" + ques.attrs['href']\n",
        "\t\t\tfile_question_urls.write(link_url+'\\n')\n",
        "\t\tfile_question_urls.close()\n",
        "\t\t\n",
        "\t\t# sleep every while in order to not get banned\n",
        "\t\tif topic_index % 5 == 4:\n",
        "\t\t\tsleep_time = (round(random.uniform(5, 10), 1))\n",
        "\t\t\ttime.sleep(sleep_time)\n",
        "\n",
        "\tbrowser.quit()\n",
        " \n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# -------------------------------------------------------------\n",
        "# answers cralwer\n",
        "def answers(urls_list,save_path):\n",
        "\tbrowser= connectchrome()\n",
        "\turl_index = -1\n",
        "\tloop_limit= len(urls_list)\n",
        "\t# output file containing all answers\n",
        "\tfile_answers = open(Path(save_path) / \"answers.txt\", mode='a') \n",
        "\tprint('Starting the answers crawling...')\n",
        "\twhile True:\n",
        "\t\turl_index += 1\n",
        "\t\tprint('--------------------------------------------------')\n",
        "\t\tif url_index >= loop_limit:\n",
        "\t\t\tprint('Crawling completed, answers have been saved to  :  ', save_path)\n",
        "\t\t\tbrowser.quit()\n",
        "\t\t\tfile_answers.close()\n",
        "\t\t\tbreak\n",
        "\t\tcurrent_line = urls_list[url_index]\n",
        "\t\tprint('processing question number  : '+ str(url_index+1))\n",
        "\t\tprint(current_line)\n",
        "\t\tif '/unanswered/' in str(current_line):\n",
        "\t\t\tprint('answer is unanswered')\n",
        "\t\t\tcontinue\n",
        "\t\tquestion_id = current_line\n",
        "\t\t# opening Question page\n",
        "\t\ttry:\n",
        "\t\t\tbrowser.get(current_line)\n",
        "\t\t\ttime.sleep(2)\n",
        "\t\texcept Exception as OpenEx:\n",
        "\t\t\tprint('cant open the following question link : ',current_line)\n",
        "\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(OpenEx).__name__, OpenEx)\n",
        "\t\t\tprint(str(OpenEx))\n",
        "\t\t\tcontinue\n",
        "\t\ttry:\n",
        "\t\t\tnb_answers_text = WebDriverWait(browser, 10).until(\n",
        "\t\t\tEC.visibility_of_element_located((By.XPATH, \"//div[text()[contains(.,'Answer')]]\"))).text\n",
        "\t\t\tnb_answers=[int(s.strip('+')) for s in nb_answers_text.split() if s.strip('+').isdigit()][0]\n",
        "\t\t\tprint('Question have :', nb_answers_text)\n",
        "\t\texcept Exception as Openans: \n",
        "\t\t\tprint('cant get answers')\n",
        "\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(Openans).__name__, Openans)\n",
        "\t\t\tprint(str(Openans))\n",
        "\t\t\tcontinue\n",
        "\t\t#nb_answers_text = browser.find_element_by_xpath(\"//div[@class='QuestionPageAnswerHeader']//div[@class='answer_count']\").text\n",
        "\t\t\n",
        "\t\tif nb_answers>7:\n",
        "\t\t\tscrolldown(browser,'answers')\n",
        "\t\tcontinue_reading_buttons = browser.find_elements_by_xpath(\"//a[@role='button']\")\n",
        "\t\ttime.sleep(2)\n",
        "\t\tfor button in continue_reading_buttons:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tActionChains(browser).click(button).perform()\n",
        "\t\t\t\ttime.sleep(1)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint('cant click more')\n",
        "\t\t\t\tcontinue\n",
        "\t\ttime.sleep(2)\n",
        "\t\thtml_source = browser.page_source\n",
        "\t\tsoup = BeautifulSoup(html_source,\"html.parser\")\n",
        "\t\t# get the question-id\n",
        "\t\tquestion_id = current_line.rsplit('/', 1)[-1]\n",
        "\t\t# find title \n",
        "\t\ttitle= current_line.replace(\"https://www.quora.com/\",\"\")\n",
        "\t\t# find question's topics\n",
        "\t\tquestions_topics= soup.findAll(\"div\", {\"class\": \"q-box qu-mr--tiny qu-mb--tiny\"})\n",
        "\t\tquestions_topics_text=[]\n",
        "\t\tfor topic in questions_topics : questions_topics_text.append(topic.text.rstrip())\n",
        "\t\t# number of answers\n",
        "\t\t# not all answers are saved!\n",
        "\t\t# answers that collapsed, and those written by annonymous users are not saved\n",
        "\t\ttry:\n",
        "\t\t\tsplit_html = html_source.split('class=\"q-box qu-pt--medium qu-pb--medium\"')\n",
        "\t\texcept Exception as notexist :#mostly because question is deleted by quora\n",
        "\t\t\tprint('question no long exists')\n",
        "\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(notexist).__name__, notexist)\n",
        "\t\t\tprint(str(notexist))\n",
        "\t\t\tcontinue\t\n",
        "\t\t# The underneath loop will generate len(split_html)/2 exceptions, cause answers in split_html\n",
        "\t\t# are eitheir in Odd or Pair positions, so ignore printed exceptions.\n",
        "\t\t#print('len split : ',len(split_html))\n",
        "\t\tfor i in range(1, len(split_html)):\n",
        "\t\t\ttry:\n",
        "\t\t\t\tpart = split_html[i]\n",
        "\t\t\t\tpart_soup = BeautifulSoup(part,\"html.parser\" )\n",
        "\t\t\t\t#print('===============================================================')\n",
        "\t\t\t\t#find users names of answers authors\n",
        "\t\t\t\ttry:\t\t\t\t\n",
        "\t\t\t\t\tauthors=part_soup.find(\"a\", href=lambda href: href and \"/profile/\" in href)\n",
        "\t\t\t\t\tuser_id = authors['href'].rsplit('/', 1)[-1]\n",
        "\t\t\t\t\t#print(user_id)\n",
        "\t\t\t\texcept Exception as notexist2 :#mostly because question is deleted by quora\n",
        "\t\t\t\t\tprint('author extract pb')\n",
        "\t\t\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(notexist2).__name__, notexist2)\n",
        "\t\t\t\t\tprint(str(notexist2))\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t# find answer dates\n",
        "\t\t\t\t\n",
        "\t\t\t\tanswer_date= part_soup.find(\"a\", string=lambda string: string and (\"Answered\" in string or \"Updated\" in string))#(\"a\", {\"class\": \"answer_permalink\"})\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\tdate=answer_date.text\n",
        "\t\t\t\t\tif (\"Updated\" in date):\n",
        "\t\t\t\t\t   date= date[8:]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t   date= date[9:]\n",
        "\t\t\t\t\tdate=dateparser.parse(date).strftime(\"%Y-%m-%d\")\n",
        "\t\t\t\texcept: # when updated or answered in the same week (ex: Updated Sat)\n",
        "\t\t\t\t\tdate=dateparser.parse(\"7 days ago\").strftime(\"%Y-%m-%d\")\n",
        "\t\t\t\t#print(date)\n",
        "\t\t\t\t# find answers text\n",
        "\t\t\t\tanswer_text = part_soup.find(\"div\", {\"class\": \"q-relative spacing_log_answer_content\"})\n",
        "\t\t\t\t#print(\" answer_text\", answer_text.text)\n",
        "\t\t\t\tanswer_text = answer_text.text\n",
        "\t\t\t\t#write answer elements to file\n",
        "\t\t\t\ts=  str(question_id.rstrip()) +'\\t' + str(date) + \"\\t\"+ user_id + \"\\t\"+ str(questions_topics_text) + \"\\t\" +\tstr(answer_text.rstrip())  + \"\\n\"\n",
        "\t\t\t\t#print(\"wrting down the answer...\")\n",
        "\t\t\t\tfile_answers.write(s)\n",
        "\t\t\t\tprint('writing down answers...')\n",
        "\t\t\texcept Exception as e1: # Most times because user is anonymous ,  continue without saving anything\n",
        "\t\t\t\tprint('---------------There is an Exception-----------')\n",
        "\t\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e1).__name__, e1)\n",
        "\t\t\t\tprint(str(e1))\n",
        "\t\t\t\to=1\n",
        "\t\t\n",
        "\t\t# we sleep every while in order to avoid IP ban\n",
        "\t\tif url_index%3==2:\n",
        "\t\t\tsleep_time=(round(random.uniform(5, 10),1))\n",
        "\t\t\ttime.sleep(sleep_time)\n",
        "\tbrowser.quit()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbTDuPEm3eP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb90d621-6321-4c08-f820-5c9004fdad26"
      },
      "source": [
        "# scraping quora answers\n",
        "# Input is a list of Questions URL and the path to save the output\n",
        "# Output is a file of scraped answers (answers.txt). An answer consists of :\n",
        "# Quest-ID | AnswerDate | AnswerAuthor-ID | Quest-tags | Answer-Text\n",
        "questions = ['https://www.quora.com/What-is-artificial-intelligence-15']\n",
        "save_path = colab_path;\n",
        "answers(questions,save_path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting the answers crawling...\n",
            "--------------------------------------------------\n",
            "processing question number  : 1\n",
            "https://www.quora.com/What-is-artificial-intelligence-15\n",
            "Question have : 100+ Answers\n",
            "scrolling down to get all answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "author extract pb\n",
            "Error on line 188 TypeError 'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "writing down answers...\n",
            "--------------------------------------------------\n",
            "Crawling completed, answers have been saved to  :   /content/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBccuA2E3eUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "9a05fe29-4d61-47d9-e3b9-a1283f3e7a22"
      },
      "source": [
        "# scraping questions related to certain topics\n",
        "# input is a list of topic keywords.\n",
        "# output is a file containing the topic's question links.\n",
        "keywords = ['semantic seo','seo']\n",
        "questions(keywords,save_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting the questions crawling\n",
            "--------------------------------------------------\n",
            "#########################################################\n",
            "Looking for topic number :  0  |  semantic seo\n",
            "topic does not have questions...\n",
            "--------------------------------------------------\n",
            "#########################################################\n",
            "Looking for topic number :  1  |  seo\n",
            "number of questions for this topic : 168000\n",
            "scrolling down to get all answers...\n",
            "--------------------------------------------------\n",
            "Crawling completed, questions have been saved to  :   /content/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isy8M2A73eY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1U7IILgwEYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}